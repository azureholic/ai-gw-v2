<policies>
    <inbound>
        <base />

        <!-- Set the backend and auth-->
        <authentication-managed-identity resource="https://cognitiveservices.azure.com" client-id="{{managed-identity-client-id}}" />
        
        <!-- Extract deployment name from request body -->
        <set-variable name="deploymentName" value="@{
            string deploymentName = &quot;N/A&quot;;
            
            try
            {
                // Extract model/deployment name from request body
                // Format: { &quot;model&quot;: &quot;deployment-name&quot;, ... }
                var body = context.Request.Body.As&lt;JObject&gt;(preserveContent: true);
                if (body != null &amp;&amp; body[&quot;model&quot;] != null)
                {
                    deploymentName = body[&quot;model&quot;].Value&lt;string&gt;();
                }
            }
            catch { }
            
            return deploymentName;
        }" />
        
        <!-- Set backend pool using deployment name with dots replaced by '-dot-' (same pools as Azure OpenAI API) -->
        <set-variable name="backendId" value="@{
            var deployment = (string)context.Variables[&quot;deploymentName&quot;];
            return deployment == &quot;N/A&quot; ? &quot;aoai-default-backend&quot; : &quot;pool-aoai-&quot; + deployment.Replace(&quot;.&quot;, &quot;-dot-&quot;).Replace(&quot; &quot;, &quot;&quot;);
        }" />
        
        <set-backend-service id="apim-generated-policy" backend-id="@((string)context.Variables[&quot;backendId&quot;])" />
        
        <!-- Add /openai/v1 prefix to the request path (Foundry natively supports /openai/v1 endpoint) -->
        <rewrite-uri template="@{return &quot;/openai/v1&quot; + context.Request.Url.Path;}" />

        <!-- Check if streaming is enabled and inject stream_options -->
        <choose>
            <when condition="@(context.Request.Body.As&lt;JObject&gt;(preserveContent: true)?[&quot;stream&quot;]?.Value&lt;bool&gt;() == true)">
                <!-- Streaming is enabled - inject stream_options to include usage -->
                <set-body>@{
                    var body = context.Request.Body.As&lt;JObject&gt;(preserveContent: true);
                    
                    // Add stream_options with include_usage: true
                    if (body["stream_options"] == null)
                    {
                        body["stream_options"] = new JObject();
                    }
                    ((JObject)body["stream_options"])["include_usage"] = true;
                    
                    return body.ToString();
                }</set-body>
            </when>
        </choose>
        
        <!-- Store whether this is a streaming request for later use -->
        <set-variable name="isStreaming" value="@(context.Request.Body.As&lt;JObject&gt;(preserveContent: true)?[&quot;stream&quot;]?.Value&lt;bool&gt;() == true)" />
        
        <!-- Capture start time for response time calculation -->
        <set-variable name="requestStartTime" value="@(DateTime.UtcNow)" />
        
        <!-- Emit Azure OpenAI token metrics to Application Insights -->
        <azure-openai-emit-token-metric namespace="AzureOpenAI">
            <dimension name="API ID" />
            <dimension name="Deployment" value="@((string)context.Variables[&quot;deploymentName&quot;])" />
            <dimension name="Streaming" value="@(((bool)context.Variables[&quot;isStreaming&quot;]).ToString())" />
            <dimension name="Request ID" value="@(context.RequestId.ToString())" />
        </azure-openai-emit-token-metric>
    </inbound>
    
    <backend>
        <base />
    </backend>
    
    <outbound>
        <base />
    </outbound>
    
    <on-error>
        <base />
        <trace source="AIUsageCapture" severity="error">@{
            return string.Format("Error capturing AI usage: {0}", context.LastError.Message);
        }</trace>
    </on-error>
</policies>