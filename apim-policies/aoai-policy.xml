<policies>
    <inbound>
        <base />

        <!-- Set the backend and auth-->
        <authentication-managed-identity resource="https://cognitiveservices.azure.com" client-id="{{managed-identity-client-id}}" />
        
        <!-- Extract deployment name from URL path -->
        <set-variable name="deploymentName" value="@{
            string deploymentName = &quot;N/A&quot;;
            
            try
            {
                // Extract deployment name from URL path
                // Format: /openai/deployments/{deployment-id}/...
                var path = context.Request.Url.Path;
                var segments = path.Split(new[] { '/' }, StringSplitOptions.RemoveEmptyEntries);
                
                for (int i = 0; i &lt; segments.Length - 1; i++)
                {
                    if (segments[i] == &quot;deployments&quot;)
                    {
                        deploymentName = System.Uri.UnescapeDataString(segments[i + 1]);
                        break;
                    }
                }
            }
            catch { }
            
            return deploymentName;
        }" />
        
        <!-- Set backend pool using deployment name with dots replaced by '-dot-' and spaces removed -->
        <set-variable name="backendId" value="@{
            var deployment = (string)context.Variables[&quot;deploymentName&quot;];
            return deployment == &quot;N/A&quot; ? &quot;aoai-default-backend&quot; : &quot;pool-&quot; + deployment.Replace(&quot;.&quot;, &quot;-dot-&quot;).Replace(&quot; &quot;, &quot;&quot;);
        }" />
        
        <set-backend-service id="apim-generated-policy" backend-id="@((string)context.Variables[&quot;backendId&quot;])" />
        
        <!-- Add /openai prefix to the request path -->
        <rewrite-uri template="@{return &quot;/openai&quot; + context.Request.Url.Path;}" />

        <!-- Check if streaming is enabled and inject stream_options -->
        <choose>
            <when condition="@(context.Request.Body.As&lt;JObject&gt;(preserveContent: true)?[&quot;stream&quot;]?.Value&lt;bool&gt;() == true)">
                <!-- Streaming is enabled - inject stream_options to include usage -->
                <set-body>@{
                    var body = context.Request.Body.As&lt;JObject&gt;(preserveContent: true);
                    
                    // Add stream_options with include_usage: true
                    if (body["stream_options"] == null)
                    {
                        body["stream_options"] = new JObject();
                    }
                    ((JObject)body["stream_options"])["include_usage"] = true;
                    
                    return body.ToString();
                }</set-body>
            </when>
        </choose>
        
        <!-- Store whether this is a streaming request for later use -->
        <set-variable name="isStreaming" value="@(context.Request.Body.As&lt;JObject&gt;(preserveContent: true)?[&quot;stream&quot;]?.Value&lt;bool&gt;() == true)" />
        
        <!-- Capture start time for response time calculation -->
        <set-variable name="requestStartTime" value="@(DateTime.UtcNow)" />
        
        <!-- Emit Azure OpenAI token metrics to Application Insights -->
        <azure-openai-emit-token-metric namespace="AzureOpenAI">
            <dimension name="API ID" />
            <dimension name="Deployment" value="@((string)context.Variables[&quot;deploymentName&quot;])" />
            <dimension name="Streaming" value="@(((bool)context.Variables[&quot;isStreaming&quot;]).ToString())" />
            <dimension name="Request ID" value="@(context.RequestId.ToString())" />
        </azure-openai-emit-token-metric>
    </inbound>
    
    <backend>
        <retry condition="@(context.Response.StatusCode == 429)" count="2" interval="1" first-fast-retry="true">
            <forward-request />
        </retry>
    </backend>
    
    <outbound>
        <base />
    </outbound>
    
    <on-error>
        <base />
        <trace source="AIUsageCapture" severity="error">@{
            return string.Format("Error capturing AI usage: {0}", context.LastError.Message);
        }</trace>
    </on-error>
</policies>
